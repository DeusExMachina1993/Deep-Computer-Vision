{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKqh+oYwn5Bly7UYNQCFjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeusExMachina1993/Deep-Computer-Vision/blob/master/Project5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0cJtBOuPu28",
        "colab_type": "text"
      },
      "source": [
        "Project5: Image Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyZ_9fwFPykd",
        "colab_type": "text"
      },
      "source": [
        "in this project we're going to learn how to classify each pixel on the image, the idea is to create a map of all detected object areas on the image. Basically what we want is the image below where every pixel has a label associated with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KTSCXh1P79T",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/ImageSegmentation.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYALDon4P-X8",
        "colab_type": "text"
      },
      "source": [
        "**Fully Convolutional network for segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBXPPQTvQAyG",
        "colab_type": "text"
      },
      "source": [
        "A Fully Convolutional neural network (FCN) is a normal CNN, where the last fully connected layer is substituted by another convolution layer with a large \"receptive field\". The idea is to capture the global context of the scene (Tell us what we have in the image and also give some very roughe idea of the locations of things)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsIxnHOQChw",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/Fully_Convolutional_Network_Semantic.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ZvZgwiQGdK",
        "colab_type": "text"
      },
      "source": [
        "It's important to remember that when we convert our last fully connected (FC) layer to a convolutional layer we gain some form of localization if we look at where we have more activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urfp0nZJQK6F",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FCN.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxjpO38hQYuI",
        "colab_type": "text"
      },
      "source": [
        "#**Conversion from normal CNN to FCN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0UUM8i8QaZb",
        "colab_type": "text"
      },
      "source": [
        "Here is how we convert a normal CNN used for classification, ie: Alexnet to a FCN used for segmentation.\n",
        "Just to remind us this is how Alexnet looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-5_ij_JQbrr",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyzMFmpuQdiI",
        "colab_type": "text"
      },
      "source": [
        "Below shows the parameters for each of the layers in AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o4rIiTgQe4n",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNklAxfVQhTG",
        "colab_type": "text"
      },
      "source": [
        "In Alexnet the inputs are fixed to be 224x224, so all the pooling effects will scale down the image from 224x224 to 55x55, 27x27, 13x13, then finally a single row vector on the FC layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCyx1Jf3QhxC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_0.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoUl77qRQkEs",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at the steps needed to do the conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UixTbthQmpf",
        "colab_type": "text"
      },
      "source": [
        "1) We start with a normal CNN for classification with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RAdTgqQQntN",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FCN_CONV_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXV7GvrkQp2c",
        "colab_type": "text"
      },
      "source": [
        "2) The second step is to convert all the FC layers to convolution layers 1x1 we don't even need to change the weights at this point. (This is already a fully convolutional neural network). The nice property of FCN networks is that we can now use any image size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HKol8ewQq0H",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FCN_CONV_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZVWGBbqQsu4",
        "colab_type": "text"
      },
      "source": [
        "Observe here that with a FCN we can use a different size H x N. The diagram bellow show a how a different size would appear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EABDUZzjQyOc",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FCN_CONV_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db-wNAaKQ1TN",
        "colab_type": "text"
      },
      "source": [
        "3) The last step is to use a \"deconv or transposed convolution\" layer to recover the activation positions to something meaningful related to the image size. Imagine that we're just scaling up the activation size to the same image size.\n",
        "This last \"upsampling\" layer also has some lernable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVP7QXXYQ17g",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FCN_CONV_4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmkmDqLMQ4Fg",
        "colab_type": "text"
      },
      "source": [
        "Now with this structure we just need to find some \"ground truth\" and to end to end learning, starting from a pre-trainned network ie: Imagenet.\n",
        "The problem with this approach is that we lose some resolution by just doing this because the activations were downscaled on a lot of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQz38IpLQ5TC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/FirstResultFCN_No_Skips.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuVdqbySQ70K",
        "colab_type": "text"
      },
      "source": [
        "To solve this problem we also get some activation from previous layers and sum/interpolate them together. This process is called \"skip\" from the creators of this algorithm.\n",
        "Even today (2016) the winners on Imagenet on the Segmentation category, used an ensemble of FCN to win the competition.\n",
        "Those up-sampling operations used on skip are also learn-able."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBI9BwT-Q85W",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/Skip_Layers_FCN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W024ivWOQ_vO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/assets/SkipExample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJshoh57RBRF",
        "colab_type": "text"
      },
      "source": [
        "Below we show the effects of this \"skip\" process, notice how the resolution of the segmentation improves after some \"skips\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_r2xPq1RCd4",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AllSkips_FCN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmQussa6RDk1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/SkipConnections.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cr-F-PvRFRa",
        "colab_type": "text"
      },
      "source": [
        "**Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNy9gnWGRGUz",
        "colab_type": "text"
      },
      "source": [
        "Another important point to note here is that the loss function we use in this image segmentation problem is actually still the usual loss function we use for classification: multi-class cross entropy and not something like the L2 loss like we would normally use when the output is an image.\n",
        "This is because despite what you might think we are actually just assigning a class to each of our output pixels so this is a classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIf0WUwzRIji",
        "colab_type": "text"
      },
      "source": [
        "#**Transposed convolution layer (deconvolution \"bad name\")**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQH0sPx6RKlz",
        "colab_type": "text"
      },
      "source": [
        "Basically the idea is to scale up, the scale down effect made on all previous layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-1nxTZARMOa",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/Conv_Deconv.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimLRAlQROJ1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/Deconv_exp.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwmbfp14RPFC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/animUpsampling.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCTQirWaRRAu",
        "colab_type": "text"
      },
      "source": [
        "It has this bad name because the upsamping forward propagation is the convolution backpropagation and the upsampling backpropagation is the convolution forward propagation.\n",
        "Also in caffe source code it is wrongly called \"deconvolution\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zztMbWdpRR9D",
        "colab_type": "text"
      },
      "source": [
        "**Extreme segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkjGxCgcRTYM",
        "colab_type": "text"
      },
      "source": [
        "There is another thing that we can do to avoid those \"skiping\" steps and also give better segmentation. Deconvnet also has better response for objects of different sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-_3nPE6RTw-",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/Deconvnet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws8_5li4RVdj",
        "colab_type": "text"
      },
      "source": [
        "This architechture is called \"Deconvnet\" which is basically another network but now with all convolution and pooling layers reversed. As you may suspect this is heavy, it takes 6 days to train on a TitanX. But the results are really good.\n",
        "Another problem is that the trainning is made in 2 stages.\n",
        "Also Deconvnets suffer less than FCN when there are small objects on the scene.\n",
        "The deconvolution network output a probability map with the same size as the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfbxvKHpRV0V",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/DeconvnetResults.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8veAodPRXmQ",
        "colab_type": "text"
      },
      "source": [
        "**Unpooling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAfziFS0RZEl",
        "colab_type": "text"
      },
      "source": [
        "Besides the deconvolution layer we also need now the unpooling layer. The max-pooling operation is non-invertible, but we can approximate, by recording the positions (Max Location switches) where we located the biggest values (during normal max-pool), then use this positions to reconstruct the data from the layer above (on this case a deconvolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCCbayWCRZZh",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/UnPoolinDiagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tnXauv1RbGv",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/Unpooling_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W20NhhGPRb2f",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/UnpoolResults.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyWMEbtaTlQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c31519e9-af59-4504-dd26-b0f558ee81e5"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOhPspI-TmuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMXJCMhETnH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "979067f9-5bd2-4fdd-f406-25d4a015d3e8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib as tfcontrib\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend as K  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSIGx4NBTpmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Upload the API token.\n",
        "def get_kaggle_credentials():\n",
        "  token_dir = os.path.join(os.path.expanduser(\"~\"),\".kaggle\")\n",
        "  token_file = os.path.join(token_dir, \"kaggle.json\")\n",
        "  if not os.path.isdir(token_dir):\n",
        "    os.mkdir(token_dir)\n",
        "  try:\n",
        "    with open(token_file,'r') as f:\n",
        "      pass\n",
        "  except IOError as no_file:\n",
        "    try:\n",
        "      from google.colab import files\n",
        "    except ImportError:\n",
        "      raise no_file\n",
        "    \n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    if \"kaggle.json\" not in uploaded:\n",
        "      raise ValueError(\"You need an API key! see: \"\n",
        "                       \"https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
        "    with open(token_file, \"wb\") as f:\n",
        "      f.write(uploaded[\"kaggle.json\"])\n",
        "    os.chmod(token_file, 600)\n",
        "\n",
        "get_kaggle_credentials()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEzlpYeRTqz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pPpwD4cTr37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "competition_name = 'carvana-image-masking-challenge'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra6JqwC0Tszt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data from Kaggle and unzip the files of interest. \n",
        "def load_data_from_zip(competition, file):\n",
        "  with zipfile.ZipFile(os.path.join(competition, file), \"r\") as zip_ref:\n",
        "    unzipped_file = zip_ref.namelist()[0]\n",
        "    zip_ref.extractall(competition)\n",
        "\n",
        "def get_data(competition):\n",
        "    kaggle.api.competition_download_files(competition, competition)\n",
        "    load_data_from_zip(competition, 'train.zip')\n",
        "    load_data_from_zip(competition, 'train_masks.zip')\n",
        "    load_data_from_zip(competition, 'train_masks.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYYh9IOhTuLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "00edb549-403b-4329-980b-ff8a8911fa97"
      },
      "source": [
        "get_data(competition_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4bcc60a22158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompetition_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m41jNQdDTvUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "6e1c2483-63d9-44b9-b341-1d66e8592d8d"
      },
      "source": [
        "img_dir = os.path.join(competition_name, \"train\")\n",
        "label_dir = os.path.join(competition_name, \"train_masks\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ab40358a93ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompetition_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompetition_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_masks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'competition_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr9tOiW6TwQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\n",
        "ids_train = df_train['img'].map(lambda s: s.split('.')[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qSLZewNTxeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_filenames = []\n",
        "y_train_filenames = []\n",
        "for img_id in ids_train:\n",
        "  x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
        "  y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUcVu5GuTyre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
        "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Wj4aXITz7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "c225a5b5-9a80-4799-8950-527e5b082172"
      },
      "source": [
        "num_train_examples = len(x_train_filenames)\n",
        "num_val_examples = len(x_val_filenames)\n",
        "\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of validation examples: {}\".format(num_val_examples))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a77636acaf92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_train_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_val_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of training examples: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of validation examples: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_val_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train_filenames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDPXBZazT0_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_filenames[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od7y38s1T2CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_filenames[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy2kVS7cT26v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_num = 5\n",
        "\n",
        "r_choices = np.random.choice(num_train_examples, display_num)\n",
        "\n",
        "plt.figure(figsize=(10, 15))\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "  x_pathname = x_train_filenames[img_num]\n",
        "  y_pathname = y_train_filenames[img_num]\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 1)\n",
        "  plt.imshow(mpimg.imread(x_pathname))\n",
        "  plt.title(\"Original Image\")\n",
        "  \n",
        "  example_labels = Image.open(y_pathname)\n",
        "  label_vals = np.unique(example_labels)\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 2)\n",
        "  plt.imshow(example_labels)\n",
        "  plt.title(\"Masked Image\")  \n",
        "  \n",
        "plt.suptitle(\"Examples of Images and their Masks\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsZlMD8JT4K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = (256, 256, 3)\n",
        "batch_size = 3\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niuE1pzvT5X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _process_pathnames(fname, label_path):\n",
        "  # We map this function onto each pathname pair  \n",
        "  img_str = tf.read_file(fname)\n",
        "  img = tf.image.decode_jpeg(img_str, channels=3)\n",
        "\n",
        "  label_img_str = tf.read_file(label_path)\n",
        "  # These are gif images so they return as (num_frames, h, w, c)\n",
        "  label_img = tf.image.decode_gif(label_img_str)[0]\n",
        "  # The label image should only have values of 1 or 0, indicating pixel wise\n",
        "  # object (car) or not (background). We take the first channel only. \n",
        "  label_img = label_img[:, :, 0]\n",
        "  label_img = tf.expand_dims(label_img, axis=-1)\n",
        "  return img, label_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxdwQQJuT68Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
        "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
        "  if width_shift_range or height_shift_range:\n",
        "      if width_shift_range:\n",
        "        width_shift_range = tf.random_uniform([], \n",
        "                                              -width_shift_range * img_shape[1],\n",
        "                                              width_shift_range * img_shape[1])\n",
        "      if height_shift_range:\n",
        "        height_shift_range = tf.random_uniform([],\n",
        "                                               -height_shift_range * img_shape[0],\n",
        "                                               height_shift_range * img_shape[0])\n",
        "      # Translate both \n",
        "      output_img = tfcontrib.image.translate(output_img,\n",
        "                                             [width_shift_range, height_shift_range])\n",
        "      label_img = tfcontrib.image.translate(label_img,\n",
        "                                             [width_shift_range, height_shift_range])\n",
        "  return output_img, label_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKbVKUL7T8LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip_img(horizontal_flip, tr_img, label_img):\n",
        "  if horizontal_flip:\n",
        "    flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
        "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
        "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
        "                                lambda: (tr_img, label_img))\n",
        "  return tr_img, label_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKbDSTyzT9TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _augment(img,\n",
        "             label_img,\n",
        "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
        "             scale=1,  # Scale image e.g. 1 / 255.\n",
        "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
        "             horizontal_flip=False,  # Random left right flip,\n",
        "             width_shift_range=0,  # Randomly translate the image horizontally\n",
        "             height_shift_range=0):  # Randomly translate the image vertically \n",
        "  if resize is not None:\n",
        "    # Resize both images\n",
        "    label_img = tf.image.resize_images(label_img, resize)\n",
        "    img = tf.image.resize_images(img, resize)\n",
        "  \n",
        "  if hue_delta:\n",
        "    img = tf.image.random_hue(img, hue_delta)\n",
        "  \n",
        "  img, label_img = flip_img(horizontal_flip, img, label_img)\n",
        "  img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
        "  label_img = tf.to_float(label_img) * scale\n",
        "  img = tf.to_float(img) * scale \n",
        "  return img, label_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTaZkhgJT-wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_baseline_dataset(filenames, \n",
        "                         labels,\n",
        "                         preproc_fn=functools.partial(_augment),\n",
        "                         threads=5, \n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True):           \n",
        "  num_x = len(filenames)\n",
        "  # Create a dataset from the filenames and labels\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "  # Map our preprocessing function to every element in our dataset, taking\n",
        "  # advantage of multithreading\n",
        "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
        "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
        "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
        "\n",
        "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
        "  \n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(num_x)\n",
        "  \n",
        "  \n",
        "  # It's necessary to repeat our data for all epochs \n",
        "  dataset = dataset.repeat().batch(batch_size)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X806F2D2T_yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "    'hue_delta': 0.1,\n",
        "    'horizontal_flip': True,\n",
        "    'width_shift_range': 0.1,\n",
        "    'height_shift_range': 0.1\n",
        "}\n",
        "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX9Lv90qUBEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "}\n",
        "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r13UHSGpUB9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a2fda6fb-bc01-4617-9372-8b24927ebf98"
      },
      "source": [
        "train_ds = get_baseline_dataset(x_train_filenames,\n",
        "                                y_train_filenames,\n",
        "                                preproc_fn=tr_preprocessing_fn,\n",
        "                                batch_size=batch_size)\n",
        "val_ds = get_baseline_dataset(x_val_filenames,\n",
        "                              y_val_filenames, \n",
        "                              preproc_fn=val_preprocessing_fn,\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2a9c7723aa17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_ds = get_baseline_dataset(x_train_filenames,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0my_train_filenames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mpreproc_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtr_preprocessing_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 batch_size=batch_size)\n\u001b[1;32m      5\u001b[0m val_ds = get_baseline_dataset(x_val_filenames,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train_filenames' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKV0tyjBUC_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}